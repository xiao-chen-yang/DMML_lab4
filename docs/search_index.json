[["index.html", "STATS5099 Data Mining and Machine Learning 1 Welcome to DMML Lab 4 1.1 Classification trees 1.2 Bagging and random forests", " STATS5099 Data Mining and Machine Learning 1 Welcome to DMML Lab 4 In week 4, we have studied three tree-based methods, namely classification trees, bagging and random forests. 1.1 Classification trees Classification trees can be implemented by using the rpart and rpart.plot packages. Some key codes are summarised below. library(rpart); library(rpart.plot) # build a tree for dataset &#39;Data&#39; with features &#39;X1&#39;, &#39;X2&#39; etc and class label &#39;Y&#39; Model &lt;- rpart(Y~X1+X2+..., data=Data, method=&quot;class&quot;) Model # visualise the tree rpart.plot(Model, type=2, extra=4) #check help page for more options about type and extra # make predictions for new data with features &#39;Xnew&#39; Ynew.pred &lt;- predict(Model, newdata=Xnew, type=&quot;class&quot;) One thing to be cautious when building the tree (or any classifier) is to avoid overfitting. This can be achieved by either setting stopping criteria to prevent tree from growing, or pruning a large tree back to simple trees. # set stopping criteria via &#39;control&#39;: smaller values of minsplit, minbucket and cp # and larger value of maxdepth lead to a larger tree Model2 &lt;- rpart(Y~X1+X2+..., data=Data, method=&quot;class&quot;, control=rpart.control(minsplit=20, minbucket=round(minsplit/3), maxdepth = 30, cp=0.01)) #values are set as default # prune the tree printcp(Model) plotcp(Model) 1.2 Bagging and random forests Bagging and random forests can be both implemented by using the randomForest function from the randomForest package. The difference is that in bagging, all features are used to build the tree for each bootstrapped sample, whereas in random forests, only a subset of features are used (set as the square root of all features by default). library(randomForest) # bagging Model &lt;- randomForest(Y~X1+X2+..., data=Data, mtry=n_feature) #n_feature is the number of features in the data # random forests Model &lt;- randomForest(Y~X1+X2+..., data=Data) "],["exercise-1-crabs-data.html", "2 Exercise 1: Crabs Data 2.1 Pruning", " 2 Exercise 1: Crabs Data In the crabs data example we are interested in predicting the sex of a crab based on its morphological measurements. This data is available in the MASS library with the following variables: sp: species - “B” or “O” for blue or orange (not used); sex: as it says (the classification variable of interest); index: index 1:50 within each of the four groups (not used); FL: frontal lobe size (mm); RW: rear width (mm); CL: carapace length (mm); CW: carapace width (mm); BD: body depth (mm). This data can be extracted from the MASS library via: library(MASS) data(crabs) We are interested in predicting the sex of the crab based on the last 5 variables, so we create a data frame of these variables to be used. data.crabs &lt;- as.data.frame(crabs[, c(2, 4:8)]) Task Split the data into training, validation and test data sets (50%, 25%, 25% for each set). Hint Check Lab 3, Section 2.2.1. Build a classification tree on the training data and plot the tree. Note down the decision rules for each node and ensure that you understand what each of the numbers mean. Hint Use the rpart and rpart.plot commands to build and visualise the classification tree. Print out the rules of the classification tree built in step 2. Again, you should understand what each of the numbers corresponds to. Based on the plot or output of the classification tree, predict the class for the following observation. ## FL RW CL CW BD ## 150 23.1 15.7 47.6 52.8 21.6 You can also check the result by using the predict function in R. predict(crabs.rt, newdata=valid.crab[1,-1],type=&quot;class&quot;) How could you determine if the classification tree is under-fitting or overfitting? Hint Which evaluation criteria may be used to assess the performance of classification tree? Calculate sensitivity and specificity for training and validation sets, assuming female is the positive class and male is the negative class. Based on results in (b), what's your conclusion about under-/over-fitting? 2.1 Pruning We may also want to reduce the size of the tree (prune it) to avoid overfitting and maximising the chance of being gerenalisable to future data. We can do this using cross-validation as below. Either look at the cross-validation complexity plot or use the output of the xerror (cross-validation error) variable. There are two strategies to choose the value of complexity parameter (and the corresponding tree size). The first option is to prune the tree back to the point where the cross-validated error is a minimum, known as the minimum error strategy. The second option is to prune the tree using the complexity parameter of the smallest tree that is within one standard deviation of the tree with the smallest xerror, known as the smallest tree strategy. In the case that there are multiple trees with the same xerror; we will choose the smaller one (since that tree would predict as well as the other one but it would also have fewer branches; thus we can also avoid overfitting). Task Build a fully grown tree. Hint Change the values of minsplit, minbucket, maxdepth and cp in rpart.control. The complexity parameter (cp) table for the fitted tree can be produced by using printcp. Given the output from a fully grown tree, decide the appropriate cp value using both the minimum error strategy and the smallest tree strategy. printcp(Full_tree) ## ## Classification tree: ## rpart(formula = sex ~ FL + RW + CL + CW + BD, data = train.crab, ## method = &quot;class&quot;, control = rpart.control(minsplit = 2, minbucket = 1, ## maxdepth = 30, cp = -1)) ## ## Variables actually used in tree construction: ## [1] BD CL FL RW ## ## Root node error: 49/100 = 0.49 ## ## n= 100 ## ## CP nsplit rel error xerror xstd ## 1 0.265306 0 1.000000 1.28571 0.098531 ## 2 0.173469 1 0.734694 0.87755 0.101036 ## 3 0.051020 4 0.204082 0.53061 0.089517 ## 4 0.030612 6 0.102041 0.48980 0.087160 ## 5 0.020408 8 0.040816 0.48980 0.087160 ## 6 0.010204 9 0.020408 0.42857 0.083124 ## 7 -1.000000 11 0.000000 0.42857 0.083124 For the following four questions, enter your answer by rounding to 3 decimal places. What is the training error rate after 4 splits? Hint The training error rate equals to rel error (relative training error rate) multiplied by root node error. What is the cross-validation error rate after 4 splits? Hint The cross-validation error rate equals to xerror multiplied by root node error. Which cp value you would choose when using the minimum error strategy? Which cp value you would choose when using the smallest tree strategy? We could also choose the cp value according to the cp plot. The minimum error strategy refers to the cp value which has the lowest X-val Relative Error. The smallest tree strategy refers to the largest cp value which is under the dashed line; the intercept of this line equals to the minimum xerror plus its standard deviation. plotcp(Full_tree) Prune the fully grown classification tree using the cp value found from the smallest tree strategy. Calculate sensitivity and specificity for training and validation sets again and compare to the results in Task 5. "],["exercise-2-titanic-data.html", "3 Exercise 2: Titanic Data", " 3 Exercise 2: Titanic Data The Titanic data can be downloaded from the titanic package in R. Once you do that, there will be two data sets loaded in R; a training data set, titanic_train, and a test data set, test_train. The dataset includes 12 variables, described as follows: PassengerID: Passenger ID Survived: Survival (0 = No; 1 = Yes) Pclass: Passenger Class (1 = 1st; 2 = 2nd; 3 = 3rd) Name: Name Sex: Sex Age: Age SibSp: Number of Siblings/Spouses Aboard Parch: Number of Parents/Children Aboard Ticket: Ticket Number Fare: Passenger Fare (British pound) Cabin: Cabin Embarked: Port of Embarkation (C = Cherbourg; Q = Queenstown; S = Southampton) Your task is to create a model that predicts who survives and who dies during the Titanic event using bagging and random forest, and compare between the two approaches. Task Perform some exploratory analysis on this data. In particular, understand the variable type, distribution of each variable, and identify which variables may be useful in predicting survival status. Hint Try str, skim, summary for numerical summaries and plot, pairs for graphical summaries. Split the training data titanic_train further into a training (80%) and validation (20%) set. This dataset includes missing data. Unlike classification trees which can construct the tree using surrogate splits, bagging and random forest cannot be directly applied to missing data. To fix it, we use the rough imputation method ,na.roughfix, provided in the randomForest package, which replaces missing values of continuous variables with their medians and missing values of categorical variables with their modes. More details can be found in the package documentation. library(randomForest) train.imputed &lt;- na.roughfix(train) #train: training data after training/validation split Apply bagging and random forest to the imputed training data (train.imputed). Which method would you prefer in this case? Hint Use the command randomForest for building a bagging and random forest classifier. Remember to change the argument mtry when applying bagging. Compare the two methods based on some appropriate evaluation criteria. Which variables are important in predicting survival under bagging and random forest? Hint Use the command varImpPlot or use ggplot as in Examples 11 and 12 in Week 4 lecture note. Use the built model to predict the test data titanic_test. Optional Task Build a classification tree on Titanic train and compare its result with Bagging and Random Forest. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
